{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aba3ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef4778",
   "metadata": {},
   "source": [
    "# Databricks Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8402ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f158cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Developer/GitHub/Lakehouse-RAG/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=[{'type': 'reasoning', 's...overned ML pipelines.'}], input_type=list])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'reasoning', 'summary': [{'type': 'summary_text', 'text': 'The user asks: \"What is MLflow?\" Provide a clear explanation. Need to describe MLflow as an open-source platform for managing the ML lifecycle, its components: tracking, projects, models, model registry, integrations, etc. Also mention use cases, architecture, benefits. Provide examples. Probably concise yet thorough.'}]}, {'type': 'text', 'text': '**MLflow** is an open‑source platform designed to simplify and standardize the end‑to‑end machine‑learning (ML) lifecycle. It was created by the Databricks team in 2018 and is now a graduated project in the Linux Foundation’s AI\\u202fFoundation.  \\n\\n---\\n\\n## Core Idea\\n\\nMLflow does **not** replace any specific ML library (e.g., scikit‑learn, TensorFlow, PyTorch).  \\nInstead, it provides a **set of loosely‑coupled components** that you can adopt incrementally to:\\n\\n1. **Track** experiments (parameters, metrics, artifacts, and source code).  \\n2. **Package** reproducible code (projects) that can run anywhere.  \\n3. **Manage** and serve models (model format & registry).  \\n4. **Deploy** models to a variety of serving environments (REST API, Spark, cloud services, etc.).\\n\\nThink of MLflow as the “glue” that gives you reliable versioning, reproducibility, and governance for your ML work, without locking you into a particular framework or cloud vendor.\\n\\n---\\n\\n## The Four Main Components\\n\\n| Component | What it does | Typical Artifacts |\\n|-----------|--------------|-------------------|\\n| **MLflow Tracking** | Centralized logging of experiments (parameters, metrics, tags, artifacts). | `run_id`, `params.json`, `metrics.csv`, model files, plots, logs |\\n| **MLflow Projects** | A standardized way to package ML code + its dependencies so others can run it with a single command. | `MLproject` file, `conda.yaml`/`requirements.txt`, source code |\\n| **MLflow Models** | A universal model packaging format (`mlflow model`) that supports multiple “flavors” (e.g., `python_function`, `torchscript`, `tensorflow`, `sklearn`). | `MLmodel` metadata, model binaries, conda environment |\\n| **MLflow Model Registry** | A collaborative hub for model versioning, stage transitions (Staging → Production → Archived), and lineage tracking. | Model versions, stage labels, comments, approval signatures |\\n\\nAll four can be used independently; you can start with just the Tracking API and add Projects, Models, and Registry later.\\n\\n---\\n\\n## How It Works – High‑Level Flow\\n\\n1. **Run an experiment**  \\n   ```python\\n   import mlflow\\n   with mlflow.start_run():\\n       mlflow.log_param(\"learning_rate\", 0.01)\\n       mlflow.log_metric(\"rmse\", 0.842)\\n       mlflow.log_artifact(\"confusion_matrix.png\")\\n   ```\\n   The run is stored in a **backend store** (SQLite, MySQL, PostgreSQL, etc.) while artifacts (model files, plots) go to an **artifact store** (local filesystem, S3, Azure Blob, GCS).\\n\\n2. **Package the code**  \\n   Create an `MLproject` file that declares entry points and environment specs. Others can run the same experiment with:  \\n   ```bash\\n   mlflow run my-repo/\\n   ```\\n\\n3. **Save the model**  \\n   ```python\\n   mlflow.sklearn.log_model(model, \"model\")\\n   ```\\n   The model is stored in the universal `mlflow` format, which can later be loaded with any supported flavor.\\n\\n4. **Register the model**  \\n   ```python\\n   result = mlflow.register_model(\\n       \"runs:/<run_id>/model\", \"my_fraud_detection\")\\n   ```\\n   The model appears in the **Model Registry**, where you can promote it to “Production”, add descriptions, or roll back to a previous version.\\n\\n5. **Serve / Deploy**  \\n   - **Local REST API**: `mlflow models serve -m models:/my_fraud_detection/Production`  \\n   - **Spark**: `mlflow.spark.deploy`  \\n   - **Docker / Kubernetes**: `mlflow models build-docker -m models:/my_fraud_detection/Production`  \\n   - **Cloud**: Direct integration with Azure ML, AWS SageMaker, GCP AI Platform, etc.\\n\\n---\\n\\n## Architectural Building Blocks\\n\\n| Piece | Role | Options |\\n|-------|------|--------|\\n| **Tracking Server** | HTTP UI + API for storing runs & metrics. | Run as a standalone service (`mlflow server`) or embed in a notebook. |\\n| **Backend Store** | Persists run metadata. | SQLite (default), MySQL, PostgreSQL, MariaDB, etc. |\\n| **Artifact Store** | Holds heavy files (models, datasets, logs). | Local path, S3, Azure Blob, GCS, HDFS, MinIO, etc. |\\n| **Model Registry Database** | Keeps model versioning metadata. | Same as backend store (usually a relational DB). |\\n| **Security & Auth** | Optional access control. | Basic auth, OAuth, LDAP, Azure AD, GCP IAM, custom proxy. |\\n\\nAll components are **stateless** except the DBs; thus they scale horizontally and can be containerized (Docker, Kubernetes) for production use.\\n\\n---\\n\\n## Why Use MLflow? (Key Benefits)\\n\\n1. **Reproducibility** – Every run records the exact code version, parameters, environment, and artifacts.\\n2. **Collaboration** – Teams can browse experiments via the web UI, comment, and compare runs side‑by‑side.\\n3. **Model Governance** – The Model Registry gives a single source of truth for which model is in production, who approved it, and when.\\n4. **Portability** – The `mlflow` model format works across frameworks and serving back‑ends.\\n5. **Flexibility** – You can plug in any storage, any compute environment, and only use the pieces you need.\\n6. **Automation‑friendly** – All actions are exposed via REST APIs and CLI, making it easy to embed in CI/CD pipelines.\\n\\n---\\n\\n## Quick Starter Example (Python)\\n\\n```python\\n# 1️⃣ Install\\n# pip install mlflow[extras]   # extras pulls optional deps for common frameworks\\n\\nimport mlflow\\nimport mlflow.sklearn\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error\\n\\n# 2️⃣ Load data\\nX, y = load_boston(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# 3️⃣ Train & log\\nwith mlflow.start_run(run_name=\"rf_boston\"):\\n    n_estimators = 200\\n    max_depth = 6\\n    mlflow.log_param(\"n_estimators\", n_estimators)\\n    mlflow.log_param(\"max_depth\", max_depth)\\n\\n    model = RandomForestRegressor(\\n        n_estimators=n_estimators,\\n        max_depth=max_depth,\\n        random_state=42)\\n    model.fit(X_train, y_train)\\n\\n    preds = model.predict(X_test)\\n    rmse = mean_squared_error(y_test, preds, squared=False)\\n    mlflow.log_metric(\"rmse\", rmse)\\n\\n    # 4️⃣ Save the model\\n    mlflow.sklearn.log_model(model, artifact_path=\"model\")\\n\\n    print(f\"Run completed. RMSE={rmse:.3f}\")\\n```\\n\\nAfter the script finishes, you can run `mlflow ui` in the same directory and explore the run, parameters, and stored model.\\n\\n---\\n\\n## Integration Points & Ecosystem\\n\\n| Tool / Platform | How it plugs into MLflow |\\n|-----------------|---------------------------|\\n| **Databricks** | Built‑in UI, auto‑logging, and managed tracking server. |\\n| **Kubeflow Pipelines** | Use `mlflow` as a step to record artifacts, then serve from registry. |\\n| **MLflow Projects** | Works with **GitHub Actions**, **GitLab CI**, **Azure DevOps**, etc., to launch reproducible runs. |\\n| **Feature Stores** (e.g., Feast) | Log the feature‑generation code as a project, store feature snapshots as artifacts. |\\n| **AutoML libraries** (AutoGluon, H2O, Optuna) | Many have native `mlflow` callbacks for automatic logging. |\\n| **Visualization** | Integrates with **TensorBoard**, **Weights & Biases**, or custom dashboards via the REST API. |\\n\\n---\\n\\n## Common Questions\\n\\n| Question | Short Answer |\\n|----------|--------------|\\n| *Do I need to rewrite my existing code?* | No. You can add a few `mlflow.log_*` calls, or enable the **auto‑logging** feature (`mlflow.autolog()`) for many frameworks. |\\n| *Can I use MLflow on a single laptop?* | Absolutely. The default SQLite backend & local `./mlruns` artifact directory work for solo development. |\\n| *Is it cloud‑agnostic?* | Yes. You pick your own storage backends (S3, Azure, GCS, etc.) and serving target. |\\n| *How does it compare to other MLOps tools (e.g., DVC, Weights & Biases, Seldon)?* | MLflow focuses on the full lifecycle (tracking → packaging → registry → deployment) and provides a **single open‑source library**. DVC is more data‑centric, W&B excels at experiment visualization, while Seldon is a specialized serving/inference layer. You can often combine them (e.g., DVC for data versioning + MLflow for model registry). |\\n| *Is there a paid enterprise version?* | Databricks offers a managed MLflow service as part of its Lakehouse platform, but the core MLflow codebase remains fully open‑source and free. |\\n\\n---\\n\\n## Getting Started Resources\\n\\n| Resource | Link |\\n|----------|------|\\n| **Official Docs** | https://mlflow.org/docs |\\n| **GitHub Repo** | https://github.com/mlflow/mlflow |\\n| **Quickstart Notebook** | https://mlflow.org/docs/latest/tutorial.html |\\n| **MLflow + Docker Compose** (self‑hosted tracking + registry) | https://github.com/mlflow/mlflow/tree/master/docker |\\n| **Community Slack** | https://mlflow.org/slack |\\n| **Blog Post** – “Machine Learning Lifecycle Management with MLflow” (Databricks) | https://databricks.com/blog/2020/09/23/mlflow-machine-learning-lifecycle-management.html |\\n\\n---\\n\\n### TL;DR\\n\\n**MLflow = an extensible, language‑agnostic framework that lets you reliably **track experiments**, **package code**, **store models**, and **manage their lifecycle** from research to production, all while staying agnostic to the underlying ML libraries and cloud providers.**  It’s a practical foundation for building reproducible, collaborative, and governed ML pipelines.'}], additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 90, 'completion_tokens': 2348, 'total_tokens': 2438}, 'prompt_tokens': 90, 'completion_tokens': 2348, 'total_tokens': 2438, 'model': 'gpt-oss-120b-080525', 'model_name': 'gpt-oss-120b-080525', 'finish_reason': 'stop'}, id='run--5e3e0831-5147-4dab-bb1b-779712547b96-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"What is MLflow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7c690c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Developer/GitHub/Lakehouse-RAG/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=[{'type': 'reasoning', 's...and from which cloud.\"}], input_type=list])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'reasoning',\n",
       "  'summary': [{'type': 'summary_text',\n",
       "    'text': 'The user asks: \"What is Unity Catalog?\" They want an answer likely about Databricks Unity Catalog, a data governance solution. Provide an overview: what it is, features, benefits, architecture, how it works, and possibly use cases. Should be concise but thorough. Also cover security, fine-grained access control, data lineage, cataloging. Mention that it provides a unified governance across all data assets, works with Lakehouse, supports multi-cloud, integrates with data policies, audit, platform. Possibly mention objects: catalogs, schemas, tables, functions, external tables. Mention how it works with IAM and external metastore, integration with notebooks, pipelines. Also mention that it\\'s a metastore for data, not just Hive metastore. Provide details on usage, APIs, and benefits.\\n\\nWill answer in a friendly explanatory style.'}]},\n",
       " {'type': 'text',\n",
       "  'text': \"**Unity Catalog** is Databricks’ unified, cross‑cloud data governance solution for the Lakehouse. It provides a single, centralized **metastore** that lets you discover, secure, and manage all of your data assets—tables, files, machine‑learning models, and even dashboards—no matter where they live (Delta\\u202fLake, other file formats, external sources, or Saa‑S services).\\n\\n---\\n\\n## Why Unity Catalog matters\\n\\n| Challenge | What Unity Catalog does |\\n|-----------|------------------------|\\n| **Fragmented metadata** | Holds a single source of truth for tables, views, functions, and notebooks across all workspaces and clouds. |\\n| **Inconsistent security** | Fine‑grained, column‑level access control (ACLs) that work across clusters, jobs, and notebooks. |\\n| **Hard‑to‑audit data usage** | Automatic data‑lineage and audit logging to Unity Catalog tables, which can be streamed to your SIEM or compliance tools. |\\n| **Multi‑cloud complexity** | A single metastore can span AWS, Azure, and GCP, so you can govern data the same way across clouds. |\\n| **Data‑sharing friction** | Secure sharing via **Unity Catalog shares** (internal cross‑account or external to partners) without copying data. |\\n| **Governance drift** | Policy‑as‑code (e.g., SQL‑based data masking, row‑level security) that lives in the catalog and is version‑controlled. |\\n\\n---\\n\\n## Core Concepts (the hierarchy)\\n\\n```\\nCatalog      ← top‑level container (e.g., “Retail”, “Finance”)\\n └─ Schema   ← logical grouping, like a database (e.g., “sales”, “hr”)\\n      └─ Table / View / External Table / Function / Materialized View\\n```\\n\\n- **Catalog**: The global namespace. You can create multiple catalogs to isolate business domains, environments (dev/prod), or regulatory boundaries.\\n- **Schema**: Traditional database‑style grouping; can also be used to separate things like “raw”, “curated”, and “analytics”.\\n- **Table/View**: All managed Delta tables, external tables (Iceberg, Parquet, Hive), and views are catalog objects.\\n- **Function**: UDFs or SQL‑based scalar/table functions that can be shared across workspaces.\\n- **Share**: A reusable object that grants read‑only access to selected tables/views to another catalog, workspace, or external consumer.\\n\\n---\\n\\n## Security & Access Control\\n\\n| Feature | Description |\\n|---------|-------------|\\n| **Fine‑grained ACLs** | Grant `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `TRUNCATE`, `CREATE`, `USAGE` at catalog, schema, table, column, or view level. |\\n| **Row‑Level Security (RLS)** | Define policies (`CREATE ROW FILTER`) that automatically filter rows based on the current user/role. |\\n| **Column‑Level Masking** | `CREATE MASK` policies to hide or transform sensitive columns (e.g., social security numbers). |\\n| **Identity Integration** | Works with Azure AD, AWS IAM, GCP IAM, and external IdPs via SCIM‑based groups. |\\n| **Audit Logging** | Every request (query, DML, DDL) is written to the `system.audit` table; events can be streamed to Unity Catalog’s own tables, a Delta Lake, or external logs (CloudTrail, Azure Monitor, GCP Audit logs). |\\n| **Data Residency & VPC Isolation** | Catalogs are tied to a specific cloud region and VPC, ensuring data never leaves the designated network. |\\n\\n---\\n\\n## Data Lineage & Catalog Intelligence\\n\\n- **Automatic lineage**: Unity Catalog captures upstream‑downstream relationships for tables, views, and notebooks, visible in the Databricks UI and via the REST API.\\n- **Metadata search**: Full‑text search across catalogs, schemas, tables, columns, tags, and comments.\\n- **Tagging & Classification**: Attach custom tags (e.g., `PII`, `GDPR`, `PCI`) to objects. Tags can be used in policy enforcement or reporting.\\n- **External Metastore Migration**: You can import existing Hive Metastore definitions or external catalog metadata (AWS Glue, Azure Purview) into Unity Catalog.\\n\\n---\\n\\n## How it fits into a Lakehouse workflow\\n\\n```mermaid\\nflowchart LR\\n    A[Ingest (Spark, Auto Loader, CDC)] --> B[Raw Zone (Delta table in “raw” schema)]\\n    B --> C[Curated Zone (Transforms, CI/CD pipelines, “curated” schema)]\\n    C --> D[Analytics (SQL, ML, BI tools)]\\n    D --> E[Secure Sharing (Unity Catalog Share)]\\n    subgraph Unity Catalog\\n        F[Catalog] --> G[Schema] --> H[Table/View]\\n    end\\n    B & C & D --> F\\n```\\n\\n1. **Ingestion** – Data lands in a Delta table that is automatically registered in a catalog/schema.\\n2. **Transformation** – Jobs reference tables by fully‑qualified names (`catalog.schema.table`); any change to policy instantly propagates to downstream jobs.\\n3. **Analytics** – Analysts, notebooks, and BI tools query the same objects with the same security guarantees.\\n4. **Sharing** – A `CREATE SHARE` statement exposes a curated subset to external partners without copying data.\\n\\n---\\n\\n## Getting Started (quick‑start steps)\\n\\n1. **Create a metastore** (if you don’t have one) – can be done through the Databricks admin console or via the `CREATE METASTORE` API.\\n2. **Assign the metastore to a workspace** – this links the workspace’s clusters to Unity Catalog.\\n3. **Create your first catalog**  \\n   ```sql\\n   CREATE CATALOG retail;\\n   ```\\n4. **Create schemas & tables**  \\n   ```sql\\n   CREATE SCHEMA retail.sales;\\n   CREATE TABLE retail.sales.transactions (\\n       txn_id STRING,\\n       amount DOUBLE,\\n       txn_date DATE,\\n       customer_id STRING\\n   )\\n   USING DELTA;\\n   ```\\n5. **Set up access** (example using a group from Azure AD)  \\n   ```sql\\n   GRANT SELECT ON CATALOG retail TO `ad-group:DataAnalysts`;\\n   GRANT MODIFY ON TABLE retail.sales.transactions TO `ad-group:DataEngineers`;\\n   ```\\n6. **Define row‑level security (optional)**  \\n   ```sql\\n   CREATE ROW FILTER retail.sales.transactions\\n   USING 'customer_region = current_user_region()';\\n   ```\\n7. **Enable audit logging** – In the admin console, toggle “Enable Unity Catalog audit logs” and point the destination bucket/container to a Delta Lake location.\\n\\n---\\n\\n## Common Use Cases\\n\\n| Use case | How Unity Catalog helps |\\n|----------|-------------------------|\\n| **Regulatory compliance (GDPR, CCPA, HIPAA)** | Column‑masking, row‑filters, immutable audit logs, and data‑classification tags. |\\n| **Cross‑team data sharing** | Shares let Team\\u202fA expose only `sales.summary` while keeping `sales.raw` private. |\\n| **Multi‑cloud data strategy** | A single catalog can span AWS S3, Azure ADLS Gen2, and GCP Cloud Storage, simplifying governance. |\\n| **ML model governance** | Register models as `TABLE FUNCTION`s or use the **Model Registry** with the same catalog permissions, ensuring model data access mirrors table access. |\\n| **Self‑service analytics** | Analysts discover tables via UI search, see lineage, and only see columns they’re allowed to view. |\\n\\n---\\n\\n## Frequently Asked Questions\\n\\n| Question | Answer |\\n|----------|--------|\\n| **Do I need to migrate existing Hive metastore?** | No, but you *can* import it (`IMPORT METASTORE`). Unity Catalog can coexist with a legacy Hive metastore for a transitional period. |\\n| **Will existing notebooks break?** | Not if you continue using the same fully‑qualified table names (`db.table`). When you move a table into a catalog, you may need to prefix it with `catalog.`. |\\n| **Is there any extra cost?** | Unity Catalog incurs metastore‑related charges (per‑hour for the metastore and per‑TB for audit logs). The exact rates are listed on the Databricks pricing page. |\\n| **Can I use it with third‑party tools (e.g., Tableau, Power\\u202fBI)?** | Yes. Those tools connect via the Databricks SQL endpoint, which enforces the same Unity Catalog policies as notebooks. |\\n| **What about data stored outside Delta (e.g., Iceberg, Parquet)?** | You can create **external tables** that reference those files, and they will still be governed by Unity Catalog ACLs and policies. |\\n\\n---\\n\\n### TL;DR\\n\\n**Unity Catalog** is Databricks’ single, cloud‑agnostic metastore that brings enterprise‑grade data governance—centralized metadata, fine‑grained security, audit logging, lineage, and secure sharing—to every layer of the Lakehouse. By registering all of your data assets in Unity Catalog, you get consistent, policy‑driven control no matter who queries the data, from which notebook, and from which cloud.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a chatbot that can answer questions about {topic}.\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "output = chain.invoke(\n",
    "    {\n",
    "        \"topic\": \"Databricks\",\n",
    "        \"question\": \"What is Unity Catalog?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "044eec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093450c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|c_custkey|            c_name|           c_address|c_nationkey|        c_phone|c_acctbal|c_mktsegment|           c_comment|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|   412445|Customer#000412445|0QAB3OjYnbP6mA0B,kgf|         21|31-421-403-4333|  5358.33|    BUILDING|arefully blithely...|\n",
      "|   412446|Customer#000412446|5u8MSbyiC7J,7PuY4...|         20|30-487-949-7942|  9441.59|   MACHINERY|sleep according t...|\n",
      "|   412447|Customer#000412447|HC4ZT62gKPgrjr ce...|          7|17-797-466-6308|  7868.75|  AUTOMOBILE|aggle blithely am...|\n",
      "|   412448|Customer#000412448|         hJok1MMrDgH|          6|16-541-510-4964|  6060.98|   MACHINERY|ly silent request...|\n",
      "|   412449|Customer#000412449|zAt1nZNG01gOhIqgy...|         14|24-710-983-5536|  4973.84|   HOUSEHOLD|refully final the...|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "import os\n",
    "\n",
    "catalog = os.environ.get(\"UC_CATALOG_NAME\")\n",
    "schema = os.environ.get(\"UC_SCHEMA_NAME\")\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "df = spark.read.table(f\"customer\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1b393d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import SparkSQLToolkit, create_spark_sql_agent\n",
    "from langchain_community.utilities.spark_sql import SparkSQL\n",
    "\n",
    "host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "\n",
    "# Note, you can also connect to Spark via Spark connect. For example:\n",
    "# db = SparkSQL.from_uri(\"sc://localhost:15002\", schema=schema)\n",
    "spark_sql = SparkSQL(schema=schema)\n",
    "toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
    "agent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True, agent_executor_kwargs={\"handle_parsing_errors\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fd1cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, so the user is asking me to describe the customer table. I'm not entirely sure if that table exists, so I should start by listing all the available tables in the database. I'll use the list_tables_sql_db tool for that. \n",
      "\n",
      "Once I have the list of tables, I can check if 'customer' is among them. If it is, the next step would be to get the schema of the customer table. The schema will tell me about the columns, their data types, and any other relevant details. \n",
      "\n",
      "I remember that the schema_sql_db tool can provide both the schema and some sample rows, which would be helpful in giving a thorough description. I need to make sure I format the input correctly as a comma-separated list, so I'll just input 'customer' since that's the only table I'm interested in right now.\n",
      "\n",
      "After getting the schema, I can then explain the structure of the customer table to the user, including the columns and their data types. This should give them a clear understanding of what the table contains.\n",
      "</think>\n",
      "\n",
      "To describe the customer table, I'll first list all available tables to confirm if 'customer' exists. Then, I'll retrieve its schema and sample rows.\n",
      "\n",
      "Action: list_tables_sql_db  \n",
      "Action Input:  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mcustomer, lineitem, nation, orders, part, partsupp, region, supplier\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The 'customer' table exists. Now, I'll get its schema and sample rows.\n",
      "\n",
      "Action: schema_sql_db  \n",
      "Action Input: customer  \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mCREATE TABLE rag_demo.dummy.customer (\n",
      "  c_custkey BIGINT,\n",
      "  c_name STRING,\n",
      "  c_address STRING,\n",
      "  c_nationkey BIGINT,\n",
      "  c_phone STRING,\n",
      "  c_acctbal DECIMAL(18,2),\n",
      "  c_mktsegment STRING,\n",
      "  c_comment STRING)\n",
      ";\n",
      "\n",
      "/*\n",
      "3 rows from customer table:\n",
      "c_custkey\tc_name\tc_address\tc_nationkey\tc_phone\tc_acctbal\tc_mktsegment\tc_comment\n",
      "412445\tCustomer#000412445\t0QAB3OjYnbP6mA0B,kgf\t21\t31-421-403-4333\t5358.33\tBUILDING\tarefully blithely regular epi\n",
      "412446\tCustomer#000412446\t5u8MSbyiC7J,7PuY4Ivaq1JRbTCMKeNVqg \t20\t30-487-949-7942\t9441.59\tMACHINERY\tsleep according to the fluffily even forges. fluffily careful packages after the ironic, silent deposi\n",
      "412447\tCustomer#000412447\tHC4ZT62gKPgrjr ceoaZgFOunlUogr7GO\t7\t17-797-466-6308\t7868.75\tAUTOMOBILE\taggle blithely among the carefully express excus\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The customer table has the following structure:\n",
      "\n",
      "- **c_custkey**: A unique identifier for the customer (BIGINT).\n",
      "- **c_name**: The name of the customer (STRING).\n",
      "- **c_address**: The address of the customer (STRING).\n",
      "- **c_nationkey**: A foreign key referencing the nation table (BIGINT).\n",
      "- **c_phone**: The phone number of the customer (STRING).\n",
      "- **c_acctbal**: The account balance of the customer (DECIMAL(18,2)).\n",
      "- **c_mktsegment**: The market segment the customer belongs to (STRING).\n",
      "- **c_comment**: Additional comments or notes about the customer (STRING).\n",
      "\n",
      "The table contains 3 sample rows with data such as customer names, addresses, nation keys, phone numbers, account balances, market segments, and comments.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The customer table contains the following columns:\n",
      "\n",
      "- **c_custkey**: A unique identifier for the customer (BIGINT).\n",
      "- **c_name**: The name of the customer (STRING).\n",
      "- **c_address**: The address of the customer (STRING).\n",
      "- **c_nationkey**: A foreign key referencing the nation table (BIGINT).\n",
      "- **c_phone**: The phone number of the customer (STRING).\n",
      "- **c_acctbal**: The account balance of the customer (DECIMAL(18,2)).\n",
      "- **c_mktsegment**: The market segment the customer belongs to (STRING).\n",
      "- **c_comment**: Additional comments or notes about the customer (STRING).\n",
      "\n",
      "The table includes data such as customer names, addresses, nation keys, phone numbers, account balances, market segments, and comments.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The customer table contains the following columns:\n",
      "\n",
      "- **c_custkey**: A unique identifier for the customer (BIGINT).\n",
      "- **c_name**: The name of the customer (STRING).\n",
      "- **c_address**: The address of the customer (STRING).\n",
      "- **c_nationkey**: A foreign key referencing the nation table (BIGINT).\n",
      "- **c_phone**: The phone number of the customer (STRING).\n",
      "- **c_acctbal**: The account balance of the customer (DECIMAL(18,2)).\n",
      "- **c_mktsegment**: The market segment the customer belongs to (STRING).\n",
      "- **c_comment**: Additional comments or notes about the customer (STRING).\n",
      "\n",
      "The table includes data such as customer names, addresses, nation keys, phone numbers, account balances, market segments, and comments.\n",
      "\n",
      "Final Answer: The customer table contains the following columns: c_custkey (BIGINT), c_name (STRING), c_address (STRING), c_nationkey (BIGINT), c_phone (STRING), c_acctbal (DECIMAL(18,2)), c_mktsegment (STRING), and c_comment (STRING). The table includes data such as customer names, addresses, nation keys, phone numbers, account balances, market segments, and comments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.run(\"Describe the customer table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b077797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The customer table contains the following columns: c_custkey (BIGINT), c_name (STRING), c_address (STRING), c_nationkey (BIGINT), c_phone (STRING), c_acctbal (DECIMAL(18,2)), c_mktsegment (STRING), and c_comment (STRING). The table includes data such as customer names, addresses, nation keys, phone numbers, account balances, market segments, and comments.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a80ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, so I need to figure out how many customers are present in the database. Let me start by understanding the tools I have and the process I should follow.\n",
      "\n",
      "First, I remember that I can use the list_tables_sql_db tool to get a list of all the tables available. That makes sense because without knowing the table names, I can't proceed. So, I'll start by using that tool with an empty input.\n",
      "\n",
      "After getting the list of tables, I need to identify which table contains customer information. Typically, such a table might be named something like \"customers\" or \"customer_list\". Once I have the correct table name, I can then query it to count the number of customers.\n",
      "\n",
      "I should also consider the schema of the table to ensure there's a column that can help me count the customers accurately. Usually, a primary key like \"customer_id\" would be present, and I can count the number of unique or non-null entries in that column.\n",
      "\n",
      "I need to make sure my SQL query is syntactically correct. Maybe I'll use the COUNT() function on the customer_id column. Something like \"SELECT COUNT(customer_id) FROM customers;\". But before executing it, I should use the query_checker_sql_db tool to verify that my query is correct and won't cause any errors.\n",
      "\n",
      "If the query is correct, I'll proceed to execute it using query_sql_db. The result should give me the total number of customers. If there's an error, I'll need to adjust my query and try again.\n",
      "\n",
      "I also need to remember not to select all columns, only the relevant ones, and limit the results if necessary, but since I'm just counting, that shouldn't be an issue here.\n",
      "\n",
      "So, step by step, I'll list the tables, find the customers table, check its schema, construct the count query, verify it, and then execute it to get the answer.\n",
      "</think>\n",
      "\n",
      "To determine the number of customers, I'll follow these steps:\n",
      "\n",
      "1. **List Tables**: Use `list_tables_sql_db` to find available tables.\n",
      "2. **Identify Customers Table**: Look for a table named \"customers\".\n",
      "3. **Check Schema**: Use `schema_sql_db` on \"customers\" to confirm the presence of a customer_id column.\n",
      "4. **Construct Query**: Write a SQL query to count the number of customers using `COUNT(customer_id)`.\n",
      "5. **Verify Query**: Use `query_checker_sql_db` to ensure the query is correct.\n",
      "6. **Execute Query**: Run the query with `query_sql_db` to get the count.\n",
      "\n",
      "**Step 1: List Tables**\n",
      "\n",
      "Action: list_tables_sql_db  \n",
      "Action Input: \"\"  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mcustomer, lineitem, nation, orders, part, partsupp, region, supplier\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The tables include \"customer\", which is likely the one containing customer data.\n",
      "\n",
      "**Step 2: Check Schema of \"customer\" Table**\n",
      "\n",
      "Action: schema_sql_db  \n",
      "Action Input: \"customer\"  \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mCREATE TABLE rag_demo.dummy.customer (\n",
      "  c_custkey BIGINT,\n",
      "  c_name STRING,\n",
      "  c_address STRING,\n",
      "  c_nationkey BIGINT,\n",
      "  c_phone STRING,\n",
      "  c_acctbal DECIMAL(18,2),\n",
      "  c_mktsegment STRING,\n",
      "  c_comment STRING)\n",
      ";\n",
      "\n",
      "/*\n",
      "3 rows from customer table:\n",
      "c_custkey\tc_name\tc_address\tc_nationkey\tc_phone\tc_acctbal\tc_mktsegment\tc_comment\n",
      "412445\tCustomer#000412445\t0QAB3OjYnbP6mA0B,kgf\t21\t31-421-403-4333\t5358.33\tBUILDING\tarefully blithely regular epi\n",
      "412446\tCustomer#000412446\t5u8MSbyiC7J,7PuY4Ivaq1JRbTCMKeNVqg \t20\t30-487-949-7942\t9441.59\tMACHINERY\tsleep according to the fluffily even forges. fluffily careful packages after the ironic, silent deposi\n",
      "412447\tCustomer#000412447\tHC4ZT62gKPgrjr ceoaZgFOunlUogr7GO\t7\t17-797-466-6308\t7868.75\tAUTOMOBILE\taggle blithely among the carefully express excus\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The \"customer\" table exists and has a column `c_custkey`, which appears to be the customer identifier. I can use this column to count the number of customers.\n",
      "\n",
      "**Step 3: Construct and Verify the Query**\n",
      "\n",
      "I'll write a SQL query to count the number of customers using the `c_custkey` column.\n",
      "\n",
      "Action: query_checker_sql_db  \n",
      "Action Input: SELECT COUNT(c_custkey) FROM customer;  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m<think>\n",
      "Okay, so I'm trying to help someone double-check their Spark SQL query. The query they provided is:\n",
      "\n",
      "SELECT COUNT(c_custkey) FROM customer;\n",
      "\n",
      "They want me to check for common mistakes like using NOT IN with NULLs, UNION vs UNION ALL, BETWEEN for exclusive ranges, data type mismatches, proper quoting, correct function arguments, casting, and proper join columns.\n",
      "\n",
      "First, I'll look at the query. It's a simple COUNT of the c_custkey column from the customer table. Let me go through each potential issue one by one.\n",
      "\n",
      "1. **NOT IN with NULLs**: The query doesn't use NOT IN at all, so that's not an issue here.\n",
      "\n",
      "2. **UNION vs UNION ALL**: There's no UNION in this query, so nothing to check here.\n",
      "\n",
      "3. **BETWEEN for exclusive ranges**: Again, no BETWEEN is used, so no problem.\n",
      "\n",
      "4. **Data type mismatch in predicates**: The query doesn't have any WHERE or FILTER clauses, so no predicates to check for data type mismatches.\n",
      "\n",
      "5. **Properly quoting identifiers**: The column name c_custkey is written without quotes. In Spark SQL, if the column name has special characters or is case-sensitive, it might need to be quoted. However, c_custkey seems like a standard identifier without special characters, so it's probably fine. But just to be safe, maybe it's better to check if the column name needs quoting based on the specific database setup.\n",
      "\n",
      "6. **Using the correct number of arguments for functions**: The COUNT function is used correctly with one argument, c_custkey. So that's correct.\n",
      "\n",
      "7. **Casting to the correct data type**: There's no casting in this query, so nothing to check here.\n",
      "\n",
      "8. **Using the proper columns for joins**: There are no joins in this query, so that's not applicable.\n",
      "\n",
      "So, the main thing I noticed is the quoting of identifiers. If the column name is case-sensitive or has special characters, it should be quoted. But in this case, c_custkey is a standard name, so it's probably okay. However, to make it more robust, maybe the user should check if the column name needs to be quoted in their specific environment.\n",
      "\n",
      "Therefore, the query seems correct as it is, but if they have specific naming conventions that require quoting, they might need to adjust it. Otherwise, the original query is fine.\n",
      "</think>\n",
      "\n",
      "The provided Spark SQL query is correct and does not contain any of the common mistakes listed. Here's the query again:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(c_custkey) FROM customer;\n",
      "```\n",
      "\n",
      "This query correctly counts the number of rows in the `customer` table where `c_custkey` is not NULL. If you need to count all rows, including those where `c_custkey` is NULL, you should use `COUNT(*)` instead.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The query is correct and ready to be executed. It will count the number of rows in the `customer` table where `c_custkey` is not NULL.\n",
      "\n",
      "**Step 4: Execute the Query**\n",
      "\n",
      "Action: query_sql_db  \n",
      "Action Input: SELECT COUNT(c_custkey) FROM customer;  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[('750000',)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "The query has successfully returned the count of customers. The result shows that there are 750,000 customers in the database.\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "There are 750,000 customers present.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**\\n\\nThere are 750,000 customers present.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = agent_executor.run(\"How many customers are present?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daaa2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lakehouse-RAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
