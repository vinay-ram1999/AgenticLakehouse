{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aba3ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef4778",
   "metadata": {},
   "source": [
    "# Databricks Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8402ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f158cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Developer/GitHub/Lakehouse-RAG/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=[{'type': 'reasoning', 's...grade ML pipelines.'}], input_type=list])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'reasoning', 'summary': [{'type': 'summary_text', 'text': 'The user asks \"What is MLflow?\" Provide an explanation. Explain MLflow as an open-source platform for managing ML lifecycle: tracking experiments, packaging code, sharing, deploying. Mention components: Tracking, Projects, Models, Model Registry. Provide details, use cases, architecture, features, comparison, maybe example code. Should be concise but thorough.'}]}, {'type': 'text', 'text': '**MLflow** is an open‑source platform (originally created by Databricks in 2018) that helps data scientists and engineers manage the entire **machine‑learning lifecycle**—from experimentation to production deployment—without locking you into any particular library, language, or cloud provider.\\n\\n---\\n\\n## Core Goals\\n\\n| Goal | Why it matters |\\n|------|----------------|\\n| **Reproducibility** | Capture code, data version, parameters, and environment so anyone can rerun a result. |\\n| **Collaboration** | Centralize experiment metadata so teams can browse, compare, and share results. |\\n| **Scalability** | Work on a single laptop, a cluster, or a cloud‑native environment with the same API. |\\n| **Portability** | Move models between frameworks (TensorFlow, PyTorch, Scikit‑Learn, XGBoost…) and deployment targets (REST API, Spark, Azure ML, AWS SageMaker, etc.). |\\n\\n---\\n\\n## The Four Primary Components\\n\\n| Component | What it does | Key APIs / Concepts |\\n|-----------|--------------|---------------------|\\n| **MLflow Tracking** | Log metrics, parameters, artifacts (files, plots, model binaries) and query them later. | `mlflow.start_run()`, `mlflow.log_param()`, `mlflow.log_metric()`, `mlflow.log_artifact()`. |\\n| **MLflow Projects** | Package code in a reusable, reproducible format (a directory with an `MLproject` file). | Supports Conda envs, Docker, or local execution; works with `mlflow run`. |\\n| **MLflow Models** | Define a **standard “flavor”** for saving a model (e.g., `python_function`, `torchscript`, `tensorflow`) and a unified way to load it. | `mlflow.<framework>.log_model()`, `mlflow.pyfunc.load_model()`. |\\n| **MLflow Model Registry** | Central catalog for model versioning, stage transitions (Staging → Production → Archived), and governance (approval, rollout). | UI + REST API for `register_model`, `transition_stage`, `add_tag`, `delete_model_version`. |\\n\\n---\\n\\n## Typical Workflow\\n\\n```python\\nimport mlflow\\nimport mlflow.sklearn\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.metrics import mean_squared_error\\nimport pandas as pd\\n\\n# 1️⃣ Start an experiment/run\\nmlflow.set_experiment(\"house-price-prediction\")\\nwith mlflow.start_run(run_name=\"rf-01\") as run:\\n    # 2️⃣ Log parameters\\n    n_estimators = 200\\n    max_depth = 12\\n    mlflow.log_param(\"n_estimators\", n_estimators)\\n    mlflow.log_param(\"max_depth\", max_depth)\\n\\n    # 3️⃣ Train model\\n    X_train, y_train = ...   # load your data\\n    model = RandomForestRegressor(n_estimators=n_estimators,\\n                                 max_depth=max_depth,\\n                                 random_state=42)\\n    model.fit(X_train, y_train)\\n\\n    # 4️⃣ Evaluate & log metrics\\n    X_val, y_val = ...\\n    preds = model.predict(X_val)\\n    rmse = mean_squared_error(y_val, preds, squared=False)\\n    mlflow.log_metric(\"rmse\", rmse)\\n\\n    # 5️⃣ Log model artifact (including the conda env)\\n    mlflow.sklearn.log_model(model,\\n                            artifact_path=\"model\",\\n                            conda_env=\"conda.yaml\")\\n\\n    # 6️⃣ (optional) Register the model in the Model Registry\\n    model_uri = f\"runs:/{run.info.run_id}/model\"\\n    mlflow.register_model(model_uri, \"HousePriceModel\")\\n```\\n\\n*All the above works locally, on a Spark cluster, or inside a Docker container—no code changes required.*\\n\\n---\\n\\n## How It Fits Into the Bigger Ecosystem\\n\\n| Ecosystem | Relationship |\\n|-----------|--------------|\\n| **Databricks** | Built‑in integration; UI, job orchestration, and automated model serving. |\\n| **Kubernetes** | Use the `mlflow` server as a Deployment; often paired with `mlflow`‑powered Helm charts for scaling. |\\n| **ML pipelines (Airflow, Prefect, Kedro, Dagster)** | MLflow runs can be embedded as tasks; tracking stays consistent across pipelines. |\\n| **Cloud services (AWS SageMaker, Azure ML, GCP Vertex AI)** | Models logged with MLflow can be *registered* then *deployed* via native cloud SDKs, or you can use MLflow’s own `mlflow models serve` and `mlflow deployments` APIs. |\\n| **Version control (Git, DVC, LakeFS)** | MLflow tracks code pointers (e.g., the Git commit SHA) automatically; DVC can store large data/artifact versions alongside. |\\n\\n---\\n\\n## Advantages vs. Alternatives\\n\\n| Feature | MLflow | Alternatives (e.g., Weights & Biases, Neptune, TensorBoard) |\\n|---------|--------|------------------------------------------------------------|\\n| **Open‑source, permissive Apache‑2.0 license** | ✔︎ | Many are SaaS‑first (W&B, Neptune) with free tiers but limited on‑premise control. |\\n| **Framework‑agnostic** | ✔︎ | TensorBoard is TensorFlow‑centric; others often need adapters. |\\n| **Built‑in Model Registry** | ✔︎ | W&B has artifacts but no native stage‑based registry. |\\n| **Packaging as Projects** | ✔︎ | Not common elsewhere; provides reproducible run environments out‑of‑the‑box. |\\n| **Easy REST/CLI + UI** | ✔︎ | All have UI/CLI, but MLflow’s UI is lightweight and can be self‑hosted with a single command. |\\n| **Hybrid deployment (local, cloud, k8s, Spark)** | ✔︎ | Some tools focus on a single environment. |\\n\\n---\\n\\n## Deployment Options\\n\\n| Deployment Mode | How to launch |\\n|-----------------|---------------|\\n| **Local server** | `mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns` |\\n| **Docker** | `docker run -p 5000:5000 -v $(pwd)/mlruns:/mlflow/mlruns -e BACKEND_STORE_URI=sqlite:///mlflow.db mlflow/mlflow` |\\n| **Kubernetes** | Helm chart `helm install mlflow mlflow/mlflow` (exposes UI, sets up PostgreSQL + S3-compatible bucket for artifacts). |\\n| **Managed (Databricks, Azure)** | One‑click “MLflow Workspace” or “Model Registry” service. |\\n\\n---\\n\\n## Common Gotchas & Tips\\n\\n| Issue | Tip |\\n|-------|-----|\\n| **Large artifacts** (e.g., model checkpoints) | Use a cloud object store (S3, GCS, Azure Blob) as the artifact root; set `MLFLOW_ARTIFACT_URI`. |\\n| **Concurrent runs racing on the same DB** | Use a robust backend store (PostgreSQL, MySQL, or Azure SQL) instead of the default SQLite. |\\n| **Environment reproducibility** | Keep the `conda.yaml` or `requirements.txt` that `mlflow.log_model` auto‑generates; version‑control it. |\\n| **Security** | Run MLflow behind an authentication proxy (e.g., OAuth2Proxy) or enable token‑based auth (available in recent MLflow releases). |\\n| **Model versioning conflict** | Use the Model Registry’s *stage* mechanism and enable “transition approval” policies to avoid accidental overwrites. |\\n\\n---\\n\\n## Quick Start Checklist\\n\\n1. **Install** – `pip install mlflow[extras]` (extras = `\"sqlalchemy\" \"s3\"` etc.).\\n2. **Pick a backend store** – SQLite for quick demos, PostgreSQL/MySQL for production.\\n3. **Pick an artifact store** – local directory for dev, S3/GCS/ADLS for scaling.\\n4. **Wrap your training code** with `mlflow.start_run()` and log everything that matters.\\n5. **Run the UI** – `mlflow ui` (defaults to `http://localhost:5000`).\\n6. **(Optional) Register** models in the Model Registry and promote them through stages.\\n7. **Deploy** – `mlflow models serve -m <model-uri>` for a quick REST endpoint, or export to your preferred serving stack.\\n\\n---\\n\\n## TL;DR\\n\\n**MLflow** is a language‑agnostic, open‑source suite that gives you:\\n\\n* **Experiment tracking** (metrics, params, artifacts)  \\n* **Reproducible packaging** of code & environments (`MLproject`)  \\n* **Standardized model saving/loading** across many ML frameworks (`mlflow models`)  \\n* **A central model registry** with versioning, stage transitions, and governance  \\n\\nAll of this is accessible via a thin Python API, a CLI, and a self‑hosted UI, making it a go‑to foundation for any team that wants to turn research notebooks into reliable, production‑grade ML pipelines.'}], additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 90, 'completion_tokens': 1986, 'total_tokens': 2076}, 'prompt_tokens': 90, 'completion_tokens': 1986, 'total_tokens': 2076, 'model': 'gpt-oss-120b-080525', 'model_name': 'gpt-oss-120b-080525', 'finish_reason': 'stop'}, id='run--aec7b5d4-5af2-4e43-8608-b97407327f5b-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"What is MLflow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c690c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Developer/GitHub/Lakehouse-RAG/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=[{'type': 'reasoning', 's...ance with Terraform).\"}], input_type=list])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'reasoning',\n",
       "  'summary': [{'type': 'summary_text',\n",
       "    'text': 'We need to answer about Unity Catalog. Provide explanation: what it is, purpose, features, architecture, security, data governance, access control, integration with Databricks, tables, schemas, catalogs, fine-grained permissions, data lineage, sharing, external locations, support for Delta Lake, etc. Also possibly mention differences to older Hive metastore, migration, best practices, roles. Provide concise yet comprehensive answer.'}]},\n",
       " {'type': 'text',\n",
       "  'text': \"**Unity Catalog** is Databricks’ unified, cloud‑native data governance solution. It provides a single logical “catalog” that spans all of your data assets—files, tables, machine‑learning models, and notebooks—across **all** Databricks workspaces, regions, and clouds that belong to the same account.  \\n\\nBelow is a quick‑reference overview of what Unity Catalog is, why it exists, and the core concepts you’ll work with.\\n\\n---\\n\\n## 1. The Problem Unity Catalog Solves\\n\\n| Traditional Approach | Pain Points |\\n|----------------------|-------------|\\n| Separate Hive Metastores per workspace, custom IAM policies, ad‑hoc file‑system ACLs | **Fragmented metadata**, inconsistent permissions, difficult cross‑workspace sharing, no global data lineage, manual audits |\\n| External tools for data discovery & governance | **Vendor lock‑in**, extra cost, sync latency, duplicate effort |\\n\\n**Unity Catalog** consolidates all of that into a **single, centrally‑managed metadata store** that is tightly integrated with Databricks, Delta Lake, and the underlying cloud provider’s IAM system.\\n\\n---\\n\\n## 2. core concepts & hierarchy\\n\\n```\\nCatalog\\n └─ Schema (aka Database)\\n     ├─ Table / View / Materialized View\\n     └─ Function\\n```\\n\\n| Level | What it represents | Typical naming guidance |\\n|------|--------------------|--------------------------|\\n| **Catalog** | Top‑level container for all data assets in a given Databricks account. You can have multiple catalogs (e.g., `analytics`, `finance`, `ml`) to isolate data domains or business units. | Use short, business‑domain names. |\\n| **Schema** | Logical grouping inside a catalog (same as a Hive database). Useful for further segmentation, e.g., `sales`, `hr`, `raw`, `curated`. | Lower‑case, underscore‑separated. |\\n| **Table/View** | Actual data objects. Tables are usually Delta Lake tables; Views are SQL‑based logical representations. | Follow your organization’s table‑naming conventions. |\\n| **Function** | User‑defined functions (UDFs) that can be shared across workspaces. | Optional, but helpful for reusable logic. |\\n\\nAll objects are **fully qualified** as `catalog.schema.object`, e.g.:\\n\\n```sql\\nSELECT * FROM finance.reporting.monthly_sales;\\n```\\n\\n---\\n\\n## 3. Unified security model\\n\\n### 3.1. Fine‑grained access control (FGAC)\\n\\n- **Principal Types**: Unity Catalog supports **users, groups, service principals, and external identities** (via SCIM or Azure AD/Okta/GCP IAM).  \\n- **Permission Types**: `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `TRUNCATE`, `CREATE`, `DROP`, `ALTER`, `USAGE`, `READ FILES`, `WRITE FILES`, `APPLY`.  \\n- **Grant hierarchy**: Permissions are inherited downwards (catalog → schema → table). You can grant at any level, and the system automatically resolves the most permissive set for a principal.\\n\\n**Example:**\\n\\n```sql\\n-- Grant read‑only access to the analytics team on the whole catalog\\nGRANT SELECT ON CATALOG analytics TO `team_analytics`;\\n\\n-- Grant write access only on a specific schema\\nGRANT INSERT, UPDATE, DELETE ON SCHEMA analytics.curated TO `team_analytics`;\\n```\\n\\n### 3.2. Integration with cloud IAM\\n\\n- **Azure**: Azure AD groups → Unity Catalog groups; access checks are enforced by the Databricks runtime, but the underlying storage ACLs are still governed by Azure RBAC.  \\n- **AWS**: IAM roles/service principals are mapped via SCIM; S3 bucket policies can be automatically generated based on Unity Catalog grants (`READ FILES`, `WRITE FILES`).  \\n- **GCP**: Cloud Identity + IAM binding; GCS bucket ACLs sync with Unity Catalog permissions.\\n\\n**Result:** A single source‑of‑truth for data permissions, no need to maintain separate bucket policies or Hive metastore ACLs.\\n\\n### 3.3. Row‑level security (RLS)\\n\\nYou can define **data masking/row filters** using SQL policies:\\n\\n```sql\\nCREATE POLICY sales_region_filter\\n  ON TABLE finance.sales\\n  USING (region = current_user()::string);\\n```\\n\\nThis policy is evaluated at query runtime, ensuring each user only sees rows they are allowed to.\\n\\n---\\n\\n## 4. Data lineage & audit\\n\\n- **Automatic lineage**: Every read/write operation against a Unity Catalog table logs a lineage edge (source → target). The UI shows a DAG of upstream/downstream tables.  \\n- **Audit logs**: Integrated with Databricks’ audit‑log service and can be streamed to your SIEM (e.g., Azure Sentinel, Splunk, CloudWatch). Logs include who ran what, from which workspace, and which objects were accessed.  \\n- **Data Explorer**: The Unity Catalog UI (in the Databricks workspace) lets you search by table name, tag, owner, or lineage depth, making data discovery fast.\\n\\n---\\n\\n## 5. Catalog‑wide data sharing\\n\\n### 5.1. Share objects between workspaces (in‑account)\\n\\nBecause the catalog lives at the **account** level, any workspace under the same account can query the same tables—no need to copy data.\\n\\n```sql\\nUSE CATALOG finance;\\nSELECT * FROM reporting.monthly_sales;\\n```\\n\\n### 5.2. External sharing (Delta Sharing)\\n\\nUnity Catalog can publish **Delta Sharing** endpoints. External partners receive a secure URL and can query the shared data directly from tools such as Snowflake, Tableau, or Python without a VPN or copy.\\n\\n- **Share definition** includes tables, views, and optional **row‑level filters**.\\n- **Revocation** is immediate: once you delete or modify the share, the external consumer loses access.\\n\\n---\\n\\n## 6. Multi‑cloud & cross‑region support\\n\\n- **Same catalog, multiple clouds**: An account can span Azure, AWS, and GCP. Unity Catalog stores its metadata in a **cloud‑agnostic service** and translates permissions to the native cloud’s storage ACLs.  \\n- **Cross‑region replication**: You can set up **external locations** that point to a bucket in another region and create **managed tables** that reference those locations. Unity Catalog respects the same FGAC rules across regions.\\n\\n---\\n\\n## 7. Migration from legacy Hive Metastore\\n\\n1. **Enable Unity Catalog** on the account (admin UI → “Account Settings → Unity Catalog”).  \\n2. **Create external locations** that point to the existing storage (S3, ADLS, GCS).  \\n3. **Run the migration wizard** (or use the `databricks uc-migrate` CLI) to import existing Hive tables as Unity Catalog tables.  \\n4. **Validate**: Compare row counts, schema, and permissions.  \\n5. **Cut over**: Update notebooks/jobs to reference `catalog.schema.table` instead of `database.table`.  \\n\\n> **Tip:** Keep the legacy metastore enabled during a phased rollout so you can revert if needed.  \\n\\n---\\n\\n## 8. Best‑practice checklist\\n\\n| ✅ Item | Why it matters |\\n|--------|----------------|\\n| **One catalog per business domain** (e.g., `marketing`, `finance`) | Keeps permissions and lineage scoped, reduces accidental cross‑domain access. |\\n| **Use groups, not individual users, for grants** | Simplifies onboarding/offboarding and audit. |\\n| **Tag sensitive tables** (`PII`, `PCI`) and enforce policies (e.g., data masking) via **SQL policies**. | Guarantees compliance (GDPR, HIPAA). |\\n| **Enable “Require MFA” for all Unity Catalog principals** via your identity provider. | Reduces credential‑theft risk. |\\n| **Regularly export audit logs** to a SIEM and set alerts for `GRANT`/`REVOKE` on privileged objects. | Early detection of privilege‑escalation. |\\n| **Set up data quality checks** (e.g., Delta Lake constraints, `expectations` framework) on each curated table. | Prevents downstream analytics from broken data. |\\n| **Document external locations** (bucket names, region) in the Unity Catalog UI. | Makes data‑ops troubleshooting painless. |\\n| **Version control your `CREATE`/`GRANT` scripts** using Terraform or `databricks cli` to enable IaC. | Guarantees reproducible environments. |\\n\\n---\\n\\n## 9. Quick “How‑to” cheat sheet\\n\\n| Goal | Command / UI |\\n|------|--------------|\\n| **Create a catalog** | `CREATE CATALOG analytics;` |\\n| **Create a schema** | `CREATE SCHEMA analytics.raw;` |\\n| **Create a Delta table** | ```sql CREATE TABLE analytics.raw.sales USING DELTA LOCATION 's3://my-bucket/raw/sales';``` |\\n| **Grant SELECT on a table** | `GRANT SELECT ON TABLE analytics.raw.sales TO `analyst_group`;` |\\n| **Create a row‑level policy** | ```sql CREATE POLICY sales_region_filter ON TABLE analytics.raw.sales USING (region = current_user()); ``` |\\n| **List lineage of a table** | UI → Data → Lineage tab, or API `GET /api/2.0/unity-catalog/lineage?table=analytics.raw.sales` |\\n| **Export audit logs** | In Account Settings → Audit Logs → “Configure sink” → choose CloudWatch / Event Hub / GCS bucket. |\\n| **Share a table via Delta Sharing** | UI → Catalog → Share → New Share → add table → generate URL. |\\n\\n---\\n\\n## 10. Bottom line\\n\\n- **Unity Catalog = one place for data metadata, governance, and security** across all cloud platforms and all Databricks workspaces in an account.  \\n- It **replaces the legacy Hive metastore** and **decouples** data permissions from the underlying storage, letting you manage access centrally via SQL‑based grants and policies.  \\n- With built‑in **lineage, audit, row‑level security, and Delta Sharing**, it gives you the compliance and data‑productivity foundation needed for modern analytics and AI workloads.\\n\\nLet me know if you’d like a deeper dive into any of the topics above (e.g., setting up row‑level policies, migrating existing tables, or automating catalog governance with Terraform).\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a chatbot that can answer questions about {topic}.\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "output = chain.invoke(\n",
    "    {\n",
    "        \"topic\": \"Databricks\",\n",
    "        \"question\": \"What is Unity Catalog?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093450c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lakehouse-RAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
