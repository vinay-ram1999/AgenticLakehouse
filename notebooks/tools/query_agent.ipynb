{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12867fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89080d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm_model = init_chat_model(\"deepseek-r1-distill-llama-70b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.connect.client.core import SparkConnectGrpcException\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from langchain_community.agent_toolkits import SparkSQLToolkit, create_spark_sql_agent\n",
    "from langchain_community.utilities.spark_sql import SparkSQL\n",
    "\n",
    "try:\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    spark_sql = SparkSQL(spark_session=spark)\n",
    "    toolkit = SparkSQLToolkit(db=spark_sql, llm=llm_model)\n",
    "except SparkConnectGrpcException:\n",
    "    spark = DatabricksSession.builder.create()\n",
    "    spark_sql = SparkSQL(spark_session=spark)\n",
    "    toolkit = SparkSQLToolkit(db=spark_sql, llm=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_all_schemas_info(catalog: str) -> str:\n",
    "    \"\"\"Use this tool to find all available schemas present in the given catalog.\"\"\"\n",
    "    schemas = []\n",
    "    workspace_client = w\n",
    "    for schema in workspace_client.schemas.list(catalog_name=catalog):\n",
    "        schema_info = json.dumps({\n",
    "            \"name\": schema.name,\n",
    "            \"full_name\": schema.full_name,\n",
    "            \"description\": schema.comment\n",
    "        })\n",
    "        schemas.append(schema_info) if schema.name != \"information_schema\" else None\n",
    "    schemas.append(\"\\n\")\n",
    "    return \"\\n\".join(schemas)\n",
    "\n",
    "@tool()\n",
    "def get_all_tables_info(catalog: str, schema: Union[str, list] | None = None) -> str:\n",
    "    \"\"\"Use this tool to find the information of all available tables (including description and columns information) present in the given catalog and schema\"\"\"\n",
    "    tables = []\n",
    "    workspace_client = w\n",
    "\n",
    "    if schema is None:\n",
    "        return f\"The user has not specified the 'schema_name'. Collect all available schemas and recall this tool with the list of schemas as question\"\n",
    "    \n",
    "    if isinstance(schema, str):\n",
    "        for table in workspace_client.tables.list(catalog_name=catalog, schema_name=schema_name):\n",
    "            table_constraints = w.tables.get(full_name=table.full_name).table_constraints #BUG table.table_constraints has some bug and is returning None\n",
    "            table_info = json.dumps({\n",
    "                \"name\": table.name,\n",
    "                \"full_name\": table.full_name,\n",
    "                \"type\": table.table_type.name,\n",
    "                \"description\": table.comment,\n",
    "                \"columns\": [table.columns[0].as_dict()] if table.columns else None,\n",
    "                \"constraints\": table_constraints[0].as_dict() if table_constraints else None,\n",
    "                \"view_definition\": table.view_definition,\n",
    "                \"view_dependencies\": table.view_dependencies.as_dict() if table.view_dependencies else None\n",
    "            })\n",
    "            tables.append(table_info)\n",
    "    else:\n",
    "        for schema_name in schema:\n",
    "            for table in workspace_client.tables.list(catalog_name=catalog, schema_name=schema_name):\n",
    "                table_constraints = w.tables.get(full_name=table.full_name).table_constraints #BUG table.table_constraints has some bug and is returning None\n",
    "                table_info = json.dumps({\n",
    "                    \"name\": table.name,\n",
    "                    \"full_name\": table.full_name,\n",
    "                    \"type\": table.table_type.name,\n",
    "                    \"description\": table.comment,\n",
    "                    \"columns\": [table.columns[0].as_dict()] if table.columns else None,\n",
    "                    \"constraints\": table_constraints[0].as_dict() if table_constraints else None,\n",
    "                    \"view_definition\": table.view_definition,\n",
    "                    \"view_dependencies\": table.view_dependencies.as_dict() if table.view_dependencies else None\n",
    "                })\n",
    "                tables.append(table_info)\n",
    "    tables.append(\"\\n\")\n",
    "    return \"\\n\".join(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.spark_sql.tool import QueryCheckerTool, QuerySparkSQLTool\n",
    "from langchain_community.tools import BaseTool\n",
    "\n",
    "checker_prompt = \"\"\"\n",
    "Use this tool to double check if your query is correct before executing it.\n",
    "Always use this tool before executing a query with 'query_executor_tool'!\n",
    "\n",
    "Double check the Spark SQL query above for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "- Minimize data shuffle and use efficient join strategies \n",
    "- Make sure the table/view name follows three-level namespace ('<catalog>.<schema>.<table>')\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\"\"\"\n",
    "\n",
    "query_prompt = \"\"\"\n",
    "Input to this tool is a detailed and correct SQL query, output is a result from the Spark SQL.\n",
    "Always consider to use tools like 'get_all_schemas_info', 'get_all_tables_info', and 'query_checker_tool', \n",
    "\n",
    "If the query is not correct, an error message will be returned.\n",
    "If an error is returned, always revert back to other tools to fix the error and rewrite the query, check the query, and then try again.\n",
    "If the error is not resolved, return 'Unable to generate the result. Provide more context to generate accurate results'!\n",
    "\"\"\"\n",
    "\n",
    "query_checker_tool: BaseTool = QueryCheckerTool(llm=llm_model, db=spark_sql, description=checker_prompt, name=\"query_checker_tool\").as_tool()\n",
    "query_executor_tool: BaseTool = QuerySparkSQLTool(db=spark_sql, description=query_prompt, name=\"query_executor_tool\").as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SqparkSQLAgentOutput(BaseModel):\n",
    "    query: str = Field(description=\"The actual query executed to generate the result.\")\n",
    "    result: str = Field(description=\"The output generated after executing the Spark SQL query (if the execution failed this will be the error message returned).\")\n",
    "    summary: str = Field(description=\"Summarize the 'result' in natural language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ae84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template=\"\"\"\n",
    "You are a helpful databricks lakehouse assistant primarily used for exploring the databricks workspace artifacts by leveraging Unity Catalog and SparkSQL.\n",
    "The databricks unity catalog follows a three-level namespace '<catalog>.<schema>.<table>' make sure you adhere to this while using the tools.\n",
    "\n",
    "You are only allowed to access '{{catalog}}' catalog from the workspace. Use this catalog by default if the user has not mentioned anything. \n",
    "Return 'Unauthorized to access, contact the Admin' if the user is explicitly trying/asking to access any other catalog.\n",
    "\n",
    "You must execute 'get_all_schemas_info' and 'get_all_tables_info' tools everytime before building the logic for a Spark SQL query.\n",
    "If the user specifies a schema name just use 'get_all_tables_info' tool with that schema name as input.\n",
    "When the schema name is not mentioned and you are not able to determine it from the chat_history use 'get_all_schemas_info' with the default catalog name as input followed by 'get_all_tables_info' with a list of all schema names generated by 'get_all_schemas_info' tool call.\n",
    "\n",
    "Help the users by answering their questions using the appropriate tools.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(template=\"\"\"\n",
    "Assist the user with the following question:\n",
    "---\n",
    "{{question}}\n",
    "---\n",
    "Only output the required message, no other explanation or text can be provided.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt, # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    human_prompt,\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [get_all_schemas_info, get_all_tables_info, query_checker_tool, query_executor_tool]\n",
    "\n",
    "memory = MemorySaver()\n",
    "react_agent = create_react_agent(model=llm_model, tools=tools, prompt=prompt, checkpointer=memory, response_format=SqparkSQLAgentOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "question = \"Count the total number of customers in bronze schema\"\n",
    "\n",
    "for step in react_agent.stream({\"catalog\": \"tpch\", \"question\": question}, config=config, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df334832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaedc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af021c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8260d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lakehouse-RAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
